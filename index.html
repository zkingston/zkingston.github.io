<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no, shrink-to-fit=no">
        <meta name="description" content="Zak Kingston">
        <meta name="author" content="Zak Kingston">
        <title>Zak Kingston</title>
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
        <link href="https://fonts.googleapis.com/css?family=Roboto:400,400i,500,500i" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Zilla+Slab:600i" rel="stylesheet">
        <script defer src="https://use.fontawesome.com/releases/v5.0.8/js/all.js"></script>
        <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

        <style>
         #sticky-sidebar {
             position:fixed;
             max-width: 20%;
         }
         html {
             height: 100%;
             width : 100%;
         }
         body {
             height     : 100%;
             width      : 100%;
             font-family: 'Roboto', sans-serif;
             font-weight: 300;
             padding-top: 4.5rem;
         }
         b {
             font-weight: 500;
         }
         h1, h2, h3, .navbar-brand {
             font-family: 'Zilla Slab', serif;
             font-weight: 600;
             font-style: italic;
             color: #405888;
         }
         h3 {
         }
         .blue {
             color: #405888;
         }
         a {
             color: #4058e8;
         }
         a:hover {
             color: #405888;
             text-decoration: none;
         }
         a.anchor {
             display: block;
             position: relative;
             top: -70px;
             visibility: hidden;
         }
         .title {
             font-size: 110%;
             font-style: italic;
         }
         .book {
             font-size: 110%;
         }
         well {
             margin: 0 auto;
             width: 100%;
         }
         .bg-light {
             background-color: #ffffff !important;
         }
        </style>
    </head>

    <body data-spy="scroll" data-target=".navbar">
        <nav class="navbar navbar-expand-lg navbar-light fixed-top bg-light">
            <a class="navbar-brand" href="#" style="font-size: 1.5em; color: #405888">Zak Kingston</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav mr-auto">
                    <li class="nav-item">
                        <a class="nav-link current" href="#about"><i class="fa fa-child"></i> About</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#news"><i class="fa fa-newspaper"></i> News</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#code"><i class="fa fa-code"></i> Code</a>
                    </li>

                    <li class="nav-item">
                        <a class="nav-link" href="#publications"><i class="fa fa-book"></i> Publications</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="http://kavrakilab.org/"><i class="fa fa-university"></i> Lab</a>
                    </li>
                </ul>
            </div>
        </nav>
        <div class="container mt-4">
            <div class="row">
                <div class="col-12 col-xs-8 col-md-9 col-lg-9 col-xl-10 order-last" style="border: 20px solid transparent;">
                    <a class="anchor" id="about"></a>
                    <h1>About</h1>
                    <div style="padding: 10px">
                        <p>
                            Starting August 2024 I will be an Assistant Professor in the <a href="https://www.cs.purdue.edu/">Department of Computer Science</a> at <a href="https://www.purdue.edu/">Purdue University</a>!
                        </p>
                        <p>
                            I am currently a postdoctoral research associate and lab manager for the <a href="http://kavrakilab.org/">Kavraki Lab</a> at <a href="http://www.cs.rice.edu/">Rice University</a> under the direction of <a href="https://www.cs.rice.edu/~kavraki/">Dr. Lydia Kavraki</a>.
                            During my Ph.D., I was funded by a NASA Space Technology Research Fellowship and worked with the Robonaut 2 team at NASA JSC.
                            My research interests are in robot motion planning and long-horizon robot autonomy, with focus on manipulation planning, planning with constraints, and hardware and software for planning.
                        </p>
                        <p>
                            More details can be found in my <a href="resources/cv_kingston.pdf">curriculum vitae <i class="fa fa-file-pdf"></i></a>.
                        </p>
                    </div>
                    <hr>
                    <a class="anchor" id="news"></a>
                    <h1>News</h1>
                    <div style="padding: 10px">
                    <dl>
                        <dt>RSS 2024</dt>
			                  <dd><p>In collaboration with <a href="https://www.claytonwramsey.com/">Clayton Ramsey</a> and <a href="https://wbthomason.github.io/">Wil Thomason</a>, the <a href="#ramsey2024">Collision-Affording Point Tree (CAPT)</a> will be presented at <a href="https://roboticsconference.org/2024/">RSS 2024</a>!
                            This work presents a novel spatial data structure for super fast (order of nanoseconds) nearest neighbor search which we use for collision checking against pointclouds.</p>
                        </dd>
                        <dt>ICRA 2024</dt>
			                  <dd><p>I had three papers accepted to <a href="https://2024.ieee-icra.org/">ICRA 2024</a>!
                            Please take a look at:
                            <ul>
                                <li><a href="#thomason2024vamp">Motions in Microseconds</a> with <a href="https://wbthomason.github.io/">Wil Thomason</a> which accelerates planning (500x over baselines!) with CPU SIMD instructions.</li>
                                <li><a href="#quintero2024impdist">Minimizing Risk with Implicit Models</a> with <a href="https://carlosquinterop.github.io/">Carlos Quintero-Peña</a> and <a href="https://wbthomason.github.io/">Wil Thomason</a> which plans risk-aware paths using learned implicit distance models.</li>
                                <li><a href="#elimelech2024skills">Learning and Using Abstract Skills</a> with <a href="http://khen.io/">Khen Elimelech</a> and <a href="https://wbthomason.github.io/">Wil Thomason</a> which speeds up task planning by learning and applying abstract skills to guide search.</li>
                            </ul>
                        </p>
                        </dd>
                        <dt>Scaling Multi-Modal Planning</dt>
			                  <dd><p>I've published a <a href="#kingston2023tro">T-RO article</a> that extends my prior work on MMP to complex systems. Now Robonaut 2 can walk across handrails!</p>
                        </dd>
                        <dt>EMPP @ IROS 2022</dt>
                        <dd><p>I co-organized the <a href="https://motion-planning-workshop.kavrakilab.org/">Evaluating Motion Planning Performance Workshop at IROS 2022</a>! Please watch the talks and read the papers of our speakers and contributors.</p>
                        </dd>
                    </dl>
                    </div>
                    <hr>

                    <a class="anchor" id="code"></a>
                    <h1>Code</h1>
                    <div style="padding: 10px">
                    <dl>
                        <dt>VAMP</dt>
                        <dd><p><a href="#thomason2024vamp">Vector-accelerated motion planning (<i>VAMP</i>)</a> is a library for sampling-based motion planning that can plan motions for high-dimensional robots in microseconds by exploiting <a href="https://en.wikipedia.org/wiki/Single_instruction,_multiple_data">CPU SIMD instructions</a>. The code is available in <a href="https://github.com/KavrakiLab/vamp">this repository</a>, and has Python bindings! The VAMP library also now supports planning against pointclouds via <a href="#ramsey2024">Collision-Affording Point Trees (CAPTs)</a>.</p>
                        </dd>
                        <dt>Robowflex</dt>
                        <dd><p><i>Robowflex</i>, a high-level C++ library for <a href="https://moveit.ros.org/">MoveIt</a> is <a href="https://github.com/KavrakiLab/robowflex">now available here</a>! Robowflex makes using MoveIt simple and has been used in a number of publications. Take a look at the <a href="#kingston2022robowflex">associated paper</a> and <a href="https://kavrakilab.github.io/robowflex/">documentation online</a>.</p>
                        </dd>
                        <dt>MotionBenchMaker</dt>
                        <dd><p><i>MotionBenchMaker</i> is a tool to generate datasets for motion planning, and contains pre-generated realistic datasets for evaluating planning. MotionBenchMaker is <a href="https://github.com/KavrakiLab/motion_bench_maker">available here</a>. Take a look at the <a href="#chamzas2021mbm">associated paper</a>.
                        <dt>Constrained Planning in OMPL</dt>
                        <dd><p>The generic constrained planning framework presented in <a href="#kingston2020isrr">ISRR 2017</a> and <a href="#kingston2019imacs">IJRR 2019</a> is now available in <a href="http://ompl.kavrakilab.org/">OMPL</a>! Take a look at the <a href="http://ompl.kavrakilab.org/constrainedPlanning.html">overview</a> and the <a href="http://ompl.kavrakilab.org/constrainedPlanningTutorial.html">tutorial</a>.
                            Now available <a href="https://moveit.picknik.ai/main/doc/how_to_guides/using_ompl_constrained_planning/ompl_constrained_planning.html">in MoveIt</a>!</p>
                        </dd>
                    </dl>
                    </div>
                    <hr>

                    <a id="publications" class="anchor"></a><h1>Publications</h1><div style="padding: 10px"><a id="journals" class="anchor"></a><h2>Peer-Reviewed Journal Articles</h2><div><a class="anchor" id="bayraktar2023"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2023</h3><dt class="col-1" style="text-align:right;">J6.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Solving Rearrangement Puzzles using Path Defragmentation in Factored State Spaces</font><br><nobr>S. Bora Bayraktar</nobr>, <nobr>Andreas Orthey</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>Marc Toussaint</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#bayraktar2023bibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#bayraktar2023abstract" data-toggle="collapse">Abstract</a><b> / </b><a href="https://arxiv.org/pdf/2212.02955.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/LRA.2023.3282788"><i class="ai ai-doi"></i> Publisher</a></p><div id="bayraktar2023abstract" class="collapse"><div class="well"><p>Rearrangement puzzles are variations of rearrangement problems in which the elements of a problem are potentially logically linked together. To efficiently solve such puzzles, we develop a motion planning approach based on a new state space that is logically factored, integrating the capabilities of the robot through factors of simultaneously manipulatable joints of an object. Based on this factored state space, we propose less-actions RRT (LA-RRT), a planner which optimizes for a low number of actions to solve a puzzle. At the core of our approach lies a new path defragmentation method, which rearranges and optimizes consecutive edges to minimize action cost. We solve six rearrangement scenarios with a Fetch robot, involving planar table puzzles and an escape room scenario. LA-RRT significantly outperforms the next best asymptotically-optimal planner by 4.01 to 6.58 times improvement in final action cost.</p></div><br><p><a href="#bayraktar2023abstract" data-toggle="collapse">Close</a></p></br></div><div id="bayraktar2023bibtex" class="collapse"><div class="well"><pre>@article{bayraktar2023,
 author = {S. Bora Bayraktar and Andreas Orthey and Zachary Kingston and Marc Toussaint and Lydia E. Kavraki},
 booktitle = {IEEE Robotics and Automation Letters},
 doi = {10.1109/LRA.2023.3282788},
 number = {8},
 pages = {4529--4536},
 title = {Solving Rearrangement Puzzles using Path Defragmentation in Factored State Spaces},
 volume = {8},
 year = {2023}
}</pre><br><p><a href="#bayraktar2023bibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="kingston2023tro"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">J5.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Scaling Multimodal Planning: Using Experience and Informing Discrete Search</font><br><nobr><b>Zachary Kingston</b></nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#kingston2023trobibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#kingston2023troabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://kavrakilab.org/publications/kingston2022-scaling-mmp.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/TRO.2022.3197080"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="#kingston2023trovideo" data-toggle="collapse"><i class="fa fa-video"></i> Video</a></p><div id="kingston2023troabstract" class="collapse"><div class="well"><p>Robotic manipulation is inherently continuous, but typically has an underlying discrete structure, such as if an object is grasped. Many problems like these are multi-modal, such as pick-and-place tasks where every object grasp and placement is a mode. Multi-modal problems require finding a sequence of transitions between modes - for example, a particular sequence of object picks and placements. However, many multi-modal planners fail to scale when motion planning is difficult (e.g., in clutter) or the task has a long horizon (e.g., rearrangement). This work presents solutions for multi-modal scalability in both these areas. For motion planning, we present an experience-based planning framework ALEF which reuses experience from similar modes both online and from training data. For task satisfaction, we present a layered planning approach that uses a discrete lead to bias search towards useful mode transitions, informed by weights over mode transitions. Together, these contributions enable multi-modal planners to tackle complex manipulation tasks that were previously infeasible or inefficient, and provide significant improvements in scenes with high-dimensional robots.</p></div><br><p><a href="#kingston2023troabstract" data-toggle="collapse">Close</a></p></br></div><div id="kingston2023trobibtex" class="collapse"><div class="well"><pre>@article{kingston2023tro,
 author = {Zachary Kingston and Lydia E. Kavraki},
 doi = {10.1109/TRO.2022.3197080},
 journal = {IEEE Transactions on Robotics},
 number = {1},
 pages = {128--146},
 title = {Scaling Multimodal Planning: Using Experience and Informing Discrete Search},
 volume = {39},
 year = {2023}
}</pre><br><p><a href="#kingston2023trobibtex" data-toggle="collapse">Close</a></p></br></div></div><div id="kingston2023trovideo" class="collapse"><div class="well"><div class="embed-responsive embed-responsive-16by9"><iframe src="https://player.vimeo.com/video/743110686?loop=1&color=ffffff&byline=0&portrait=0" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><br><p><a href="#kingston2023trovideo" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="chamzas2021mbm"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2021</h3><dt class="col-1" style="text-align:right;">J4.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">MotionBenchMaker: A Tool to Generate and Benchmark Motion Planning Datasets</font><br><nobr>Constantinos Chamzas</nobr>, <nobr>Carlos Quintero-Peña</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>Andreas Orthey</nobr>, <nobr>Daniel Rakita</nobr>, <nobr>Michael Gleicher</nobr>, <nobr>Marc Toussaint</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#chamzas2021mbmbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#chamzas2021mbmabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://kavrakilab.org/publications/chamzas2022-motion-bench-maker.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/LRA.2021.3133603"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="https://www.youtube.com/watch?v=t96Py0QX0NI"><i class="fa fa-video"></i> Video</a><b> / </b><a href="https://www.youtube.com/watch?v=dCxXZWGQIlQ"><i class="fa fa-video"></i> Talk</a></p><div id="chamzas2021mbmabstract" class="collapse"><div class="well"><p>Recently, there has been a wealth of development in motion planning for robotic manipulationnew motion planners are continuously proposed, each with its own unique set of strengths and weaknesses. However, evaluating these new planners is challenging, and researchers often create their own ad-hoc problems for benchmarking, which is time-consuming, prone to bias, and does not directly compare against other state-of-the-art planners. We present MotionBenchMaker, an open-source tool to generate benchmarking datasets for realistic robot manipulation problems. MotionBenchMaker is designed to be an extensible, easy-to-use tool that allows users to both generate datasets and benchmark them by comparing motion planning algorithms. Empirically, we show the benefit of using MotionBenchMaker as a tool to procedurally generate datasets which helps in the fair evaluation of planners. We also present a suite of over 40 prefabricated datasets, with 5 different commonly used robots in 8 environments, to serve as a common ground for future motion planning research.</p></div><br><p><a href="#chamzas2021mbmabstract" data-toggle="collapse">Close</a></p></br></div><div id="chamzas2021mbmbibtex" class="collapse"><div class="well"><pre>@article{chamzas2021mbm,
 author = {Constantinos Chamzas and Carlos Quintero-Peña and Zachary Kingston and Andreas Orthey and Daniel Rakita and Michael Gleicher and Marc Toussaint and Lydia E. Kavraki},
 doi = {10.1109/LRA.2021.3133603},
 journal = {IEEE Robotics and Automation Letters},
 number = {2},
 pages = {882--889},
 title = {MotionBenchMaker: A Tool to Generate and Benchmark Motion Planning Datasets},
 volume = {7},
 year = {2021},
 youtube = {https://www.youtube.com/watch?v=t96Py0QX0NI}
}</pre><br><p><a href="#chamzas2021mbmbibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="kingston2019imacs"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2019</h3><dt class="col-1" style="text-align:right;">J3.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Exploring Implicit Spaces for Constrained Sampling-Based Planning</font><br><nobr><b>Zachary Kingston</b></nobr>, <nobr>Mark Moll</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#kingston2019imacsbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#kingston2019imacsabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://kavrakilab.org/publications/kingston2019exploring-implicit-spaces-for-constrained.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1177/0278364919868530"><i class="ai ai-doi"></i> Publisher</a></p><div id="kingston2019imacsabstract" class="collapse"><div class="well"><p>We present a review and reformulation of manifold constrained sampling-based motion planning within a unifying framework, IMACS (implicit manifold configuration space). IMACS enables a broad class of motion planners to plan in the presence of manifold constraints, decoupling the choice of motion planning algorithm and method for constraint adherence into orthogonal choices. We show that implicit configuration spaces defined by constraints can be presented to sampling-based planners by addressing two key fundamental primitives, sampling and local planning, and that IMACS preserves theoretical properties of probabilistic completeness and asymptotic optimality through these primitives. Within IMACS, we implement projection- and continuation-based methods for constraint adherence, and demonstrate the framework on a range of planners with both methods in simulated and realistic scenarios. Our results show that the choice of method for constraint adherence depends on many factors and that novel combinations of planners and methods of constraint adherence can be more effective than previous approaches. Our implementation of IMACS is open source within the Open Motion Planning Library and is easily extended for novel planners and constraint spaces.</p></div><br><p><a href="#kingston2019imacsabstract" data-toggle="collapse">Close</a></p></br></div><div id="kingston2019imacsbibtex" class="collapse"><div class="well"><pre>@article{kingston2019imacs,
 author = {Zachary Kingston and Mark Moll and Lydia E. Kavraki},
 doi = {10.1177/0278364919868530},
 journal = {The International Journal of Robotics Research},
 month = {9},
 number = {10--11},
 pages = {1151--1178},
 title = {Exploring Implicit Spaces for Constrained Sampling-Based Planning},
 volume = {38},
 year = {2019}
}</pre><br><p><a href="#kingston2019imacsbibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="dantam2018tmp"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2018</h3><dt class="col-1" style="text-align:right;">J2.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">An Incremental Constraint-Based Framework for Task and Motion Planning</font><br><nobr>Neil T. Dantam</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>Swarat Chaudhuri</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#dantam2018tmpbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#dantam2018tmpabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://kavrakilab.org/publications/dantam2018incremental-tmp.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1177/0278364918761570"><i class="ai ai-doi"></i> Publisher</a></p><div id="dantam2018tmpabstract" class="collapse"><div class="well"><p>We present a new algorithm for task and motion planning (TMP) and discuss the requirements and abstrations necessary to obtain robust solutions for TMP in general. Our Iteratively Deepened Task and Motion Planning (IDTMP) method is probabilistically-complete and offers improved performance and generality compared to a similar, state-of-the-art, probabilistically-complete planner. The key idea of IDTMP is to leverage incremental constraint solving to efficiently add and remove constraints on motion feasibility at the task level. We validate IDTMP on a physical manipulator and evaluate scalability on scenarios with many objects and long plans, showing order-of-magnitude gains compared to the benchmark planner and a four-times self-comparison speedup from our extensions. Finally, in addition to describing a new method for TMP and its implementation on a physical robot, we also put forward requirements and abstractions for the development of similar planners in the future.</p></div><br><p><a href="#dantam2018tmpabstract" data-toggle="collapse">Close</a></p></br></div><div id="dantam2018tmpbibtex" class="collapse"><div class="well"><pre>@article{dantam2018tmp,
 author = {Neil T. Dantam and Zachary Kingston and Swarat Chaudhuri and Lydia E. Kavraki},
 doi = {10.1177/0278364918761570},
 journal = {The International Journal of Robotics Research, Special Issue on the 2016 Robotics: Science and Systems Conference},
 number = {10},
 pages = {1134--1151},
 title = {An Incremental Constraint-Based Framework for Task and Motion Planning},
 volume = {37},
 year = {2018}
}</pre><br><p><a href="#dantam2018tmpbibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="kingston2018ar"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">J1.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Sampling-Based Methods for Motion Planning with Constraints</font><br><nobr><b>Zachary Kingston</b></nobr>, <nobr>Mark Moll</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#kingston2018arbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#kingston2018arabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://kavrakilab.org/publications/kingston2018sampling-based-methods-for-motion-planning.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1146/annurev-control-060117-105226"><i class="ai ai-doi"></i> Publisher</a></p><div id="kingston2018arabstract" class="collapse"><div class="well"><p>Robots with many degrees of freedom (e.g., humanoid robots and mobile manipulators) have increasingly been employed to accomplish realistic tasks in domains such as disaster relief, spacecraft logistics, and home caretaking. Finding feasible motions for these robots autonomously is essential for their operation. Sampling-based motion planning algorithms have been shown to be effective for these high-dimensional systems. However, incorporating task constraints (e.g., keeping a cup level, writing on a board) into the planning process introduces significant challenges. is survey describes the families of methods for sampling-based planning with constraints and places them on a spectrum delineated by their complexity. Constrained sampling-based methods are based upon two core primitive operations: (1) sampling constraint-satisfying configurations and (2) generating constraint-satisfying continuous motion. Although the basics of sampling-based planning are presented for contextual background, the survey focuses on the representation of constraints and sampling-based planners that incorporate constraints.</p></div><br><p><a href="#kingston2018arabstract" data-toggle="collapse">Close</a></p></br></div><div id="kingston2018arbibtex" class="collapse"><div class="well"><pre>@article{kingston2018ar,
 author = {Zachary Kingston and Mark Moll and Lydia E. Kavraki},
 doi = {10.1146/annurev-control-060117-105226},
 journal = {Annual Review of Control, Robotics, and Autonomous Systems},
 number = {1},
 pages = {159--185},
 title = {Sampling-Based Methods for Motion Planning with Constraints},
 volume = {1},
 year = {2018}
}</pre><br><p><a href="#kingston2018arbibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl></div><a id="conference" class="anchor"></a><h2>Peer-Reviewed Conference Papers</h2><div><a class="anchor" id="ramsey2024"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2024</h3><dt class="col-1" style="text-align:right;">C20.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Collision-Affording Point Trees: SIMD-Amenable Nearest Neighbors for Fast Collision Checking</font><br><nobr>Clayton W. Ramsey</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>Wil Thomason</nobr>, and <nobr>Lydia E. Kavraki</nobr></br> (Equal Contribution. To Appear.)</p><p><a href="#ramsey2024bibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#ramsey2024abstract" data-toggle="collapse">Abstract</a><b> / </b><a href="https://arxiv.org/abs/2406.02807"><i class="fa fa-link"></i> Publisher</a><b> / </b><a href="https://www.youtube.com/watch?v=BzDKdrU1VpM"><i class="fa fa-video"></i> Video</a></p><div id="ramsey2024abstract" class="collapse"><div class="well"><p>Motion planning against sensor data is often a critical bottleneck in real-time robot control. For sampling-based motion planners, which are effective for high-dimensional systems such as manipulators, the most time-intensive component is collision checking. We present a novel spatial data structure, the collision-affording point tree (CAPT): an exact representation of point clouds that accelerates collision-checking queries between robots and point clouds by an order of magnitude, with an average query time of less than 10 nanoseconds on 3D scenes comprising thousands of points. With the CAPT, sampling-based planners can generate valid, high-quality paths in under a millisecond, with total end-to-end computation time faster than 60 FPS, on a single thread of a consumer-grade CPU. We also present a point cloud filtering algorithm, based on space-filling curves, which reduces the number of points in a point cloud while preserving structure. Our approach enables robots to plan at real-time speeds in sensed environments, opening up potential uses of planning for high-dimensional systems in dynamic, changing, and unmodeled environments.</p></div><br><p><a href="#ramsey2024abstract" data-toggle="collapse">Close</a></p></br></div><div id="ramsey2024bibtex" class="collapse"><div class="well"><pre>@inproceedings{ramsey2024,
 author = {Clayton W. Ramsey and Zachary Kingston and Wil Thomason and Lydia E. Kavraki},
 booktitle = {Robotics: Science and Systems},
 title = {Collision-Affording Point Trees: SIMD-Amenable Nearest Neighbors for Fast Collision Checking},
 url = {https://arxiv.org/abs/2406.02807},
 year = {2024},
 youtube = {https://www.youtube.com/watch?v=BzDKdrU1VpM}
}</pre><br><p><a href="#ramsey2024bibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="thomason2024vamp"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">C19.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Motions in Microseconds via Vectorized Sampling-Based Planning</font><br><nobr>Wil Thomason</nobr>, <nobr><b>Zachary Kingston</b></nobr>, and <nobr>Lydia E. Kavraki</nobr></br> (Equal Contribution.)</p><p><a href="#thomason2024vampbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#thomason2024vampabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="https://arxiv.org/pdf/2309.14545.pdf"><i class="fa fa-file-pdf"></i> PDF</a></p><div id="thomason2024vampabstract" class="collapse"><div class="well"><p>Modern sampling-based motion planning algorithms typically take between hundreds of milliseconds to dozens of seconds to find collision-free motions for high degree-of-freedom problems. This paper presents performance improvements of more than 500x over the state-of-the-art, bringing planning times into the range of microseconds and solution rates into the range of kilohertz, without specialized hardware. Our key insight is how to exploit fine-grained parallelism within sampling-based planners, providing generality-preserving algorithmic improvements to any such planner and significantly accelerating critical subroutines, such as forward kinematics and collision checking. We demonstrate our approach over a diverse set of challenging, realistic problems for complex robots ranging from 7 to 14 degrees-of-freedom. Moreover, we show that our approach does not require high-power hardware by also evaluating on a low-power single-board computer. The planning speeds demonstrated are fast enough to reside in the range of control frequencies and open up new avenues of motion planning research.</p></div><br><p><a href="#thomason2024vampabstract" data-toggle="collapse">Close</a></p></br></div><div id="thomason2024vampbibtex" class="collapse"><div class="well"><pre>@inproceedings{thomason2024vamp,
 author = {Wil Thomason and Zachary Kingston and Lydia E. Kavraki},
 booktitle = {IEEE International Conference on Robotics and Automation},
 title = {Motions in Microseconds via Vectorized Sampling-Based Planning},
 year = {2024}
}</pre><br><p><a href="#thomason2024vampbibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="quintero2024impdist"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">C18.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Stochastic Implicit Neural Signed Distance Functions for Safe Motion Planning under Sensing Uncertainty</font><br><nobr>Carlos Quintero-Peña</nobr>, <nobr>Wil Thomason</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>Anastasios Kyrillidis</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#quintero2024impdistbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#quintero2024impdistabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="https://arxiv.org/pdf/2309.16862.pdf"><i class="fa fa-file-pdf"></i> PDF</a></p><div id="quintero2024impdistabstract" class="collapse"><div class="well"><p>Motion planning under sensing uncertainty is critical for robots in unstructured environments to guarantee safety for both the robot and any nearby humans. Most work on planning under uncertainty does not scale to high-dimensional robots such as manipulators, assumes simplified geometry of the robot or environment, or requires per-object knowledge of noise. Instead, we propose a method that directly models sensor-specific aleatoric uncertainty to find safe motions for high-dimensional systems in complex environments, without exact knowledge of environment geometry. We combine a novel implicit neural model of stochastic signed distance functions with a hierarchical optimization-based motion planner to plan low-risk motions without sacrificing path quality. Our method also explicitly bounds the risk of the path, offering trustworthiness. We empirically validate that our method produces safe motions and accurate risk bounds and is safer than baseline approaches.</p></div><br><p><a href="#quintero2024impdistabstract" data-toggle="collapse">Close</a></p></br></div><div id="quintero2024impdistbibtex" class="collapse"><div class="well"><pre>@inproceedings{quintero2024impdist,
 author = {Carlos Quintero-Peña and Wil Thomason and Zachary Kingston and Anastasios Kyrillidis and Lydia E. Kavraki},
 booktitle = {IEEE International Conference on Robotics and Automation},
 title = {Stochastic Implicit Neural Signed Distance Functions for Safe Motion Planning under Sensing Uncertainty},
 year = {2024}
}</pre><br><p><a href="#quintero2024impdistbibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="elimelech2024skills"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">C17.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Accelerating long-horizon planning with affordance-directed dynamic grounding of abstract skills</font><br><nobr>Khen Elimelech</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>Wil Thomason</nobr>, <nobr>Moshe Y. Vardi</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#elimelech2024skillsbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#elimelech2024skillsabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://khen.io/icra24appendix.pdf"><i class="fa fa-file-pdf"></i> PDF</a></p><div id="elimelech2024skillsabstract" class="collapse"><div class="well"><p>Long-horizon task planning is important for robot autonomy, especially as a subroutine for frameworks such as Integrated Task and Motion Planning. However, task planning is computationally challenging and struggles to scale to realistic problem settings. We propose to accelerate task planning over an agent's lifetime by integrating learned abstract skills: a generalizable planning experience encoding introduced in earlier work. In this work, we contribute a practical approach to planning with skills by introducing a novel formalism of planning in a skill-augmented domain. We also introduce and formulate the notion of a skill's affordance, which indicates its predicted benefit to the solution, and use it to guide the planning and skill grounding processes. Together, our observations yield an affordance-directed, lazy-search planning algorithm, which can seamlessly compose skills and actions to solve long-horizon planning problems. We evaluate our planner in an object rearrangement domain, where we demonstrate performance benefits relative to a state-of-the-art task planner.</p></div><br><p><a href="#elimelech2024skillsabstract" data-toggle="collapse">Close</a></p></br></div><div id="elimelech2024skillsbibtex" class="collapse"><div class="well"><pre>@inproceedings{elimelech2024skills,
 author = {Khen Elimelech and Zachary Kingston and Wil Thomason and Moshe Y. Vardi and Lydia E. Kavraki},
 booktitle = {IEEE International Conference on Robotics and Automation},
 title = {Accelerating long-horizon planning with affordance-directed dynamic grounding of abstract skills},
 year = {2024}
}</pre><br><p><a href="#elimelech2024skillsbibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="shome2023privacy"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2023</h3><dt class="col-1" style="text-align:right;">C16.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Robots as AI Double Agents: Privacy in Motion Planning</font><br><nobr>Rahul Shome</nobr>, <nobr><b>Zachary Kingston</b></nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#shome2023privacybibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#shome2023privacyabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="https://arxiv.org/pdf/2308.03385.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/IROS55552.2023.10341460"><i class="ai ai-doi"></i> Publisher</a></p><div id="shome2023privacyabstract" class="collapse"><div class="well"><p>Robotics and automation are poised to change the landscape of home and work in the near future. Robots are adept at deliberately moving, sensing, and interacting with their environments. The pervasive use of this technology promises societal and economic payoffs due to its capabilities - conversely, the capabilities of robots to move within and sense the world around them is susceptible to abuse. Robots, unlike typical sensors, are inherently autonomous, active, and deliberate. Such automated agents can become AI double agents liable to violate the privacy of coworkers, privileged spaces, and other stakeholders. In this work we highlight the understudied and inevitable threats to privacy that can be posed by the autonomous, deliberate motions and sensing of robots. We frame the problem within broader sociotechnological questions alongside a comprehensive review. The privacy-aware motion planning problem is formulated in terms of cost functions that can be modified to induce privacy-aware behavior - preserving, agnostic, or violating. Simulated case studies in manipulation and navigation, with altered cost functions, are used to demonstrate how privacy-violating threats can be easily injected, sometimes with only small changes in performance (solution path lengths). Such functionality is already widely available. This preliminary work is meant to lay the foundations for near-future, holistic, interdisciplinary investigations that can address questions surrounding privacy in intelligent robotic behaviors determined by planning algorithms.</p></div><br><p><a href="#shome2023privacyabstract" data-toggle="collapse">Close</a></p></br></div><div id="shome2023privacybibtex" class="collapse"><div class="well"><pre>@inproceedings{shome2023privacy,
 author = {Rahul Shome and Zachary Kingston and Lydia E. Kavraki},
 booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/IROS55552.2023.10341460},
 number = {},
 pages = {2861-2868},
 title = {Robots as AI Double Agents: Privacy in Motion Planning},
 volume = {},
 year = {2023}
}</pre><br><p><a href="#shome2023privacybibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="lee2023physics"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">C15.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Object Reconfiguration with Simulation-Derived Feasible Actions</font><br><nobr>Yiyuan Lee</nobr>, <nobr>Wil Thomason</nobr>, <nobr><b>Zachary Kingston</b></nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#lee2023physicsbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#lee2023physicsabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="https://kavrakilab.org/publications/lee2023-simulation-actions.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/ICRA48891.2023.10160377"><i class="ai ai-doi"></i> Publisher</a></p><div id="lee2023physicsabstract" class="collapse"><div class="well"><p>3D object reconfiguration encompasses common robot manipulation tasks in which a set of objects must be moved through a series of physically feasible state changes into a desired final configuration. Object reconfiguration is challenging to solve in general, as it requires efficient reasoning about environment physics that determine action validity. This information is typically manually encoded in an explicit transition system. Constructing these explicit encodings is tedious and error-prone, and is often a bottleneck for planner use. In this work, we explore embedding a physics simulator within a motion planner to implicitly discover and specify the valid actions from any state, removing the need for manual specification of action semantics. Our experiments demonstrate that the resulting simulation-based planner can effectively produce physically valid rearrangement trajectories for a range of 3D object reconfiguration problems without requiring more than an environment description and start and goal arrangements.</p></div><br><p><a href="#lee2023physicsabstract" data-toggle="collapse">Close</a></p></br></div><div id="lee2023physicsbibtex" class="collapse"><div class="well"><pre>@inproceedings{lee2023physics,
 author = {Yiyuan Lee and Wil Thomason and Zachary Kingston and Lydia E. Kavraki},
 booktitle = {IEEE International Conference on Robotics and Automation},
 doi = {10.1109/ICRA48891.2023.10160377},
 number = {},
 pages = {8104-8111},
 title = {Object Reconfiguration with Simulation-Derived Feasible Actions},
 volume = {},
 year = {2023}
}</pre><br><p><a href="#lee2023physicsbibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="quintero2023optimal"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">C14.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Optimal Grasps and Placements for Task and Motion Planning in Clutter</font><br><nobr>Carlos Quintero-Peña</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>Tianyang Pan</nobr>, <nobr>Rahul Shome</nobr>, <nobr>Anastasios Kyrillidis</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#quintero2023optimalbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#quintero2023optimalabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="https://kavrakilab.org/publications/quintero2023-optimal-tmp.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/ICRA48891.2023.10161455"><i class="ai ai-doi"></i> Publisher</a></p><div id="quintero2023optimalabstract" class="collapse"><div class="well"><p>Many methods that solve robot planning problems, such as task and motion planners, employ discrete symbolic search to find sequences of valid symbolic actions that are grounded with motion planning. Much of the efficacy of these planners lies in this grounding—bad placement and grasp choices can lead to inefficient planning when a problem has many geometric constraints. Moreover, grounding methods such as naı̈ve sampling often fail to find appropriate values for these choices in the presence of clutter. Towards efficient task and motion planning, we present a novel optimization-based approach for grounding to solve cluttered problems that have many constraints that arise from geometry. Our approach finds an optimal grounding and can provide feedback to discrete search for more effective planning. We demonstrate our method against baseline methods in complex simulated environments.</p></div><br><p><a href="#quintero2023optimalabstract" data-toggle="collapse">Close</a></p></br></div><div id="quintero2023optimalbibtex" class="collapse"><div class="well"><pre>@inproceedings{quintero2023optimal,
 author = {Carlos Quintero-Peña and Zachary Kingston and Tianyang Pan and Rahul Shome and Anastasios Kyrillidis and Lydia E. Kavraki},
 booktitle = {IEEE International Conference on Robotics and Automation},
 doi = {10.1109/ICRA48891.2023.10161455},
 number = {},
 pages = {3707-3713},
 title = {Optimal Grasps and Placements for Task and Motion Planning in Clutter},
 volume = {},
 year = {2023}
}</pre><br><p><a href="#quintero2023optimalbibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="kingston2022robowflex"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2022</h3><dt class="col-1" style="text-align:right;">C13.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Robowflex: Robot Motion Planning with MoveIt Made Easy</font><br><nobr><b>Zachary Kingston</b></nobr>, and <nobr>Lydia E. Kavraki</nobr></br> (Nominated for Best Paper for Industrial Robotics Research for Practicality.)</p><p><a href="#kingston2022robowflexbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#kingston2022robowflexabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="https://kavrakilab.org/publications/kingston2022-robowflex.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/IROS47612.2022.9981698"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="#kingston2022robowflexvideo" data-toggle="collapse"><i class="fa fa-video"></i> Video</a><b> / </b><a href="https://www.youtube.com/watch?v=mPDE3QSkLJ0"><i class="fa fa-video"></i> Talk</a></p><div id="kingston2022robowflexabstract" class="collapse"><div class="well"><p>Robowflex is a software library for robot motion planning in industrial and research applications, leveraging the popular MoveIt library and Robot Operating System (ROS) middleware. Robowflex takes advantage of the ease of motion planning with MoveIt while providing an augmented API to craft and manipulate motion planning queries within a single program. Robowflex's high-level API simplifies many common use-cases while still providing access to the underlying MoveIt library. Robowflex is particularly useful for 1) developing new motion planners, 2) evaluation of motion planners, and 3) complex problems that use motion planning (e.g., task and motion planning). Robowflex also provides visualization capabilities, integrations to other robotics libraries (e.g., DART and Tesseract), and is complimentary to many other robotics packages. With our library, the user does not need to be an expert at ROS or MoveIt in order to set up motion planning queries, extract information from results, and directly interface with a variety of software components. We provide a few example use-cases that demonstrate its efficacy.</p></div><br><p><a href="#kingston2022robowflexabstract" data-toggle="collapse">Close</a></p></br></div><div id="kingston2022robowflexbibtex" class="collapse"><div class="well"><pre>@inproceedings{kingston2022robowflex,
 author = {Zachary Kingston and Lydia E. Kavraki},
 booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/IROS47612.2022.9981698},
 pages = {3108--3114},
 title = {Robowflex: Robot Motion Planning with MoveIt Made Easy},
 year = {2022}
}</pre><br><p><a href="#kingston2022robowflexbibtex" data-toggle="collapse">Close</a></p></br></div></div><div id="kingston2022robowflexvideo" class="collapse"><div class="well"><div class="embed-responsive embed-responsive-16by9"><iframe src="https://player.vimeo.com/video/760062092?h=68a0b835cf&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><br><p><a href="#kingston2022robowflexvideo" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="kingston2021"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2021</h3><dt class="col-1" style="text-align:right;">C12.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Using Experience to Improve Constrained Planning on Foliations for Multi-Modal Problems</font><br><nobr><b>Zachary Kingston</b></nobr>, <nobr>Constantinos Chamzas</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#kingston2021bibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#kingston2021abstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://www.kavrakilab.org/publications/kingston2021experience-foliations.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/IROS51168.2021.9636236"><i class="ai ai-doi"></i> Publisher</a></p><div id="kingston2021abstract" class="collapse"><div class="well"><p>Many robotic manipulation problems are multi-modal—they consist of a discrete set of mode families (e.g., whether an object is grasped or placed) each with a continuum of parameters (e.g., where exactly an object is grasped). Core to these problems is solving single-mode motion plans, i.e., given a mode from a mode family (e.g., a specific grasp), find a feasible motion to transition to the next desired mode. Many planners for such problems have been proposed, but complex manipulation plans may require prohibitively long computation times due to the difficulty of solving these underlying single-mode problems. It has been shown that using experience from similar planning queries can significantly improve the efficiency of motion planning. However, even though modes from the same family are similar, they impose different constraints on the planning problem, and thus experience gained in one mode cannot be directly applied to another. We present a new experience-based framework, ALEF , for such multi-modal planning problems. ALEF learns using paths from single-mode problems from a mode family, and applies this experience to novel modes from the same family. We evaluate ALEF on a variety of challenging problems and show a significant improvement in the efficiency of sampling-based planners both in isolation and within a multi-modal manipulation planner.</p></div><br><p><a href="#kingston2021abstract" data-toggle="collapse">Close</a></p></br></div><div id="kingston2021bibtex" class="collapse"><div class="well"><pre>@inproceedings{kingston2021,
 author = {Zachary Kingston and Constantinos Chamzas and Lydia E. Kavraki},
 booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/IROS51168.2021.9636236},
 pages = {6922--6927},
 title = {Using Experience to Improve Constrained Planning on Foliations for Multi-Modal Problems},
 year = {2021}
}</pre><br><p><a href="#kingston2021bibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="moll2021hyper"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">C11.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">HyperPlan: A Framework for Motion Planning Algorithm Selection and Parameter Optimization</font><br><nobr>Mark Moll</nobr>, <nobr>Constantinos Chamzas</nobr>, <nobr><b>Zachary Kingston</b></nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#moll2021hyperbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#moll2021hyperabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://kavrakilab.org/publications/moll2021hyperplan.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/IROS51168.2021.9636651"><i class="ai ai-doi"></i> Publisher</a></p><div id="moll2021hyperabstract" class="collapse"><div class="well"><p>Over the years, many motion planning algorithms have been proposed. It is often unclear which algorithm might be best suited for a particular class of problems. The problem is compounded by the fact that algorithm performance can be highly dependent on parameter settings. This paper shows that hyperparameter optimization is an effective tool in both algorithm selection and parameter tuning over a given set of motion planning problems. We present different loss functions for optimization that capture different notions of optimality. The approach is evaluated on a broad range of scenes using two different manipulators, a Fetch and a Baxter. We show that optimized planning algorithm performance significantly improves upon baseline performance and generalizes broadly in the sense that performance improvements carry over to problems that are very different from the ones considered during optimization.</p></div><br><p><a href="#moll2021hyperabstract" data-toggle="collapse">Close</a></p></br></div><div id="moll2021hyperbibtex" class="collapse"><div class="well"><pre>@inproceedings{moll2021hyper,
 author = {Mark Moll and Constantinos Chamzas and Zachary Kingston and Lydia E. Kavraki},
 booktitle = {IEEE/RSJ International Conference on Intelligent Robots and Systems},
 doi = {10.1109/IROS51168.2021.9636651},
 pages = {2511-2518},
 title = {HyperPlan: A Framework for Motion Planning Algorithm Selection and Parameter Optimization},
 year = {2021}
}</pre><br><p><a href="#moll2021hyperbibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="chamzas2021flame"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">C10.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Learning Sampling Distributions Using Local 3D Workspace Decompositions for Motion Planning in High Dimensions</font><br><nobr>Constantinos Chamzas</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>Carlos Quintero-Peña</nobr>, <nobr>Anshumali Shrivastava</nobr>, and <nobr>Lydia E. Kavraki</nobr></br> (Nominated for Best Paper in Cognitive Robotics.)</p><p><a href="#chamzas2021flamebibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#chamzas2021flameabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://www.kavrakilab.org/publications/chamzas2021-learn-sampling.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/ICRA48506.2021.9561104"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="https://www.youtube.com/watch?v=cH4_lIjjs58"><i class="fa fa-video"></i> Video</a></p><div id="chamzas2021flameabstract" class="collapse"><div class="well"><p>Earlier work has shown that reusing experience from prior motion planning problems can improve the efficiency of similar, future motion planning queries. However, for robots with many degrees-of-freedom, these methods exhibit poor generalization across different environments and often require large datasets that are impractical to gather. We present SPARK and FLAME, two experience-based frameworks for sampling- based planning applicable to complex manipulators in 3D environments. Both combine samplers associated with features from a workspace decomposition into a global biased sampling distribution. SPARK decomposes the environment based on exact geometry while FLAME is more general, and uses an octree-based decomposition obtained from sensor data. We demonstrate the effectiveness of SPARK and FLAME on a real and simulated Fetch robot tasked with challenging pick-and-place manipulation problems. Our approaches can be trained incrementally and significantly improve performance with only a handful of examples, generalizing better over diverse tasks and environments as compared to prior approaches.</p></div><br><p><a href="#chamzas2021flameabstract" data-toggle="collapse">Close</a></p></br></div><div id="chamzas2021flamebibtex" class="collapse"><div class="well"><pre>@inproceedings{chamzas2021flame,
 author = {Constantinos Chamzas and Zachary Kingston and Carlos Quintero-Peña and Anshumali Shrivastava and Lydia E. Kavraki},
 booktitle = {IEEE International Conference on Robotics and Automation},
 doi = {10.1109/ICRA48506.2021.9561104},
 pages = {1283--1289},
 title = {Learning Sampling Distributions Using Local 3D Workspace Decompositions for Motion Planning in High Dimensions},
 year = {2021},
 youtube = {https://www.youtube.com/watch?v=cH4_lIjjs58}
}</pre><br><p><a href="#chamzas2021flamebibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="wells2021icra"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">C9.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Finite Horizon Synthesis for Probabilistic Manipulation Domains</font><br><nobr>Andrew M. Wells</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>Morteza Lahijanian</nobr>, <nobr>Lydia E. Kavraki</nobr>, and <nobr>Moshe Y. Vardi</nobr></br></p><p><a href="#wells2021icrabibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#wells2021icraabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://www.kavrakilab.org/publications/wells2021-finite-horizon-synthesis.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/ICRA48506.2021.9561297"><i class="ai ai-doi"></i> Publisher</a></p><div id="wells2021icraabstract" class="collapse"><div class="well"><p>Robots have begun operating and collaborating with humans in industrial and social settings. This collaboration introduces challenges: the robot must plan while taking the human’s actions into account. In prior work, the problem was posed as a 2-player deterministic game, with a limited number of human moves. The limit on human moves is unintuitive, and in many settings determinism is undesirable. In this paper, we present a novel planning method for collaborative human-robot manipulation tasks via probabilistic synthesis. We introduce a probabilistic manipulation domain that captures the interaction by allowing for both robot and human actions with states that represent the configurations of the objects in the workspace. The task is specified using Linear Temporal Logic over finite traces (LTLf). We then transform our manipulation domain into a Markov Decision Process (MDP) and synthesize an optimal policy to satisfy the specification on this MDP. We present two novel contributions: a formalization of probabilistic manipulation domains allowing us to apply existing techniques and a comparison of different encodings of these domains. Our framework is validated on a physical UR5 robot.</p></div><br><p><a href="#wells2021icraabstract" data-toggle="collapse">Close</a></p></br></div><div id="wells2021icrabibtex" class="collapse"><div class="well"><pre>@inproceedings{wells2021icra,
 author = {Andrew M. Wells and Zachary Kingston and Morteza Lahijanian and Lydia E. Kavraki and Moshe Y. Vardi},
 booktitle = {IEEE International Conference on Robotics and Automation},
 doi = {10.1109/ICRA48506.2021.9561297},
 pages = {6336--6342},
 title = {Finite Horizon Synthesis for Probabilistic Manipulation Domains},
 year = {2021}
}</pre><br><p><a href="#wells2021icrabibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="kingston2020leads"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2020</h3><dt class="col-1" style="text-align:right;">C8.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Informing Multi-Modal Planning with Synergistic Discrete Leads</font><br><nobr><b>Zachary Kingston</b></nobr>, <nobr>Andrew M. Wells</nobr>, <nobr>Mark Moll</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#kingston2020leadsbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#kingston2020leadsabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://kavrakilab.org/publications/kingston2020weighting-multi-modal-leads.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/ICRA40945.2020.9197545"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="#kingston2020leadsvideo" data-toggle="collapse"><i class="fa fa-video"></i> Video</a></p><div id="kingston2020leadsabstract" class="collapse"><div class="well"><p>Robotic manipulation problems are inherently continuous, but typically have underlying discrete structure, e.g., whether or not an object is grasped. This means many problems are multi-modal and in particular have a continuous infinity of modes. For example, in a pick-and-place manipulation domain, every grasp and placement of an object is a mode. Usually manipulation problems require the robot to transition into different modes, e.g., going from a mode with an object placed to another mode with the object grasped. To successfully find a manipulation plan, a planner must find a sequence of valid single-mode motions as well as valid transitions between these modes. Many manipulation planners have been proposed to solve tasks with multi-modal structure. However, these methods require mode-specific planners and fail to scale to very cluttered environments or to tasks that require long sequences of transitions. This paper presents a general layered planning approach to multi-modal planning that uses a discrete "lead" to bias search towards useful mode transitions. The difficulty of achieving specific mode transitions is captured online and used to bias search towards more promising sequences of modes. We demonstrate our planner on complex scenes and show that significant performance improvements are tied to both our discrete "lead" and our continuous representation.</p></div><br><p><a href="#kingston2020leadsabstract" data-toggle="collapse">Close</a></p></br></div><div id="kingston2020leadsbibtex" class="collapse"><div class="well"><pre>@inproceedings{kingston2020leads,
 author = {Zachary Kingston and Andrew M. Wells and Mark Moll and Lydia E. Kavraki},
 booktitle = {IEEE International Conference on Robotics and Automation},
 doi = {10.1109/ICRA40945.2020.9197545},
 pages = {3199--3205},
 title = {Informing Multi-Modal Planning with Synergistic Discrete Leads},
 year = {2020}
}</pre><br><p><a href="#kingston2020leadsbibtex" data-toggle="collapse">Close</a></p></br></div></div><div id="kingston2020leadsvideo" class="collapse"><div class="well"><div class="embed-responsive embed-responsive-16by9"><iframe src="https://player.vimeo.com/video/393316293?loop=1&color=ffffff&byline=0&portrait=0" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><br><p><a href="#kingston2020leadsvideo" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="kingston2020isrr"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">C7.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Decoupling Constraints from Sampling-Based Planners</font><br><nobr><b>Zachary Kingston</b></nobr>, <nobr>Mark Moll</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#kingston2020isrrbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#kingston2020isrrabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://kavrakilab.org/publications/kingston2017decoupling-constraints.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1007/978-3-030-28619-4_62"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="#kingston2020isrrvideo" data-toggle="collapse"><i class="fa fa-video"></i> Video</a></p><div id="kingston2020isrrabstract" class="collapse"><div class="well"><p>We present a general unifying framework for sampling-based motion planning under kinematic task constraints which enables a broad class of planners to compute plans that satisfy a given constraint function that encodes, e.g., loop closure, balance, and end-effector constraints. The framework decouples a planner’s method for exploration from constraint satisfaction by representing the implicit configuration space defined by a constraint function. We emulate three constraint satisfaction methodologies from the literature, and demonstrate the framework with a range of planners utilizing these constraint methodologies. Our results show that the appropriate choice of constrained satisfaction methodology depends on many factors, e.g., the dimension of the configuration space and implicit constraint manifold, and number of obstacles. Furthermore, we show that novel combinations of planners and constraint satisfaction methodologies can be more effective than previous approaches. The framework is also easily extended for novel planners and constraint spaces.</p></div><br><p><a href="#kingston2020isrrabstract" data-toggle="collapse">Close</a></p></br></div><div id="kingston2020isrrbibtex" class="collapse"><div class="well"><pre>@incollection{kingston2020isrr,
 address = {Cham},
 author = {Zachary Kingston and Mark Moll and Lydia E. Kavraki},
 booktitle = {Robotics Research},
 doi = {10.1007/978-3-030-28619-4_62},
 editor = {Amato, N. M. and Hager, G. and Thomas, S. and Torres-Torriti, M.},
 isbn = {978-3-030-28619-4},
 pages = {913--928},
 publisher = {Springer International Publishing},
 title = {Decoupling Constraints from Sampling-Based Planners},
 year = {2020}
}</pre><br><p><a href="#kingston2020isrrbibtex" data-toggle="collapse">Close</a></p></br></div></div><div id="kingston2020isrrvideo" class="collapse"><div class="well"><div class="embed-responsive embed-responsive-16by9"><iframe src="https://player.vimeo.com/video/261052837?loop=1&color=ffffff&byline=0&portrait=0" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><br><p><a href="#kingston2020isrrvideo" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="habibi2018dars"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2018</h3><dt class="col-1" style="text-align:right;">C6.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Distributed Object Characterization with Local Sensing by a Multi-Robot System</font><br><nobr>Golnaz Habibi</nobr>, <nobr>S&aacute;ndor P. Fekete</nobr>, <nobr><b>Zachary Kingston</b></nobr>, and <nobr>James McLurkin</nobr></br></p><p><a href="#habibi2018darsbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#habibi2018darsabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="https://s3.amazonaws.com/zk-bucket/rsc/Habibi2018.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1007/978-3-319-73008-0_15"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="#habibi2018darsvideo" data-toggle="collapse"><i class="fa fa-video"></i> Video</a></p><div id="habibi2018darsabstract" class="collapse"><div class="well"><p>This paper presents two distributed algorithms for enabling a swarm of robots with local sensing and local coordinates to estimate the dimensions and orientation of an unknown complex polygonal object, ie, its minimum and maximum width and its main axis. Our first approach is based on a robust heuristic of distributed Principal Component Analysis (DPCA), while the second is based on turning the idea of Rotating Calipers into a distributed algorithm (DRC). We simulate DRC and DPCA methods and test DPCA on real robots. The result show our algorithms successfully estimate the dimension and orientation of convex or concave objects with a reasonable error in the presence of noisy data.</p></div><br><p><a href="#habibi2018darsabstract" data-toggle="collapse">Close</a></p></br></div><div id="habibi2018darsbibtex" class="collapse"><div class="well"><pre>@incollection{habibi2018dars,
 author = {Golnaz Habibi and S{\'a}ndor P. Fekete and Zachary Kingston and James McLurkin},
 booktitle = {Distributed Autonomous Robotic Systems},
 doi = {10.1007/978-3-319-73008-0_15},
 editor = {Roderich Gro{\ss} and Andreas Kolling and Spring Berman and Emilio Frazzoli and Alcherio Martinoli and Fumitoshi Matsuno and Melvin Gauci},
 pages = {205--218},
 publisher = {Springer Proceedings in Advanced Robotics},
 title = {Distributed Object Characterization with Local Sensing by a Multi-Robot System},
 volume = {6},
 year = {2018}
}</pre><br><p><a href="#habibi2018darsbibtex" data-toggle="collapse">Close</a></p></br></div></div><div id="habibi2018darsvideo" class="collapse"><div class="well"><div class="embed-responsive embed-responsive-16by9"><iframe src="https://player.vimeo.com/video/287250201?loop=1&color=ffffff&byline=0&portrait=0" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><br><p><a href="#habibi2018darsvideo" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="baker2017r2"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2017</h3><dt class="col-1" style="text-align:right;">C5.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Robonaut 2 and You: Specifying and Executing Complex Operations</font><br><nobr>William Baker</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>Mark Moll</nobr>, <nobr>Julia Badger</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#baker2017r2bibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#baker2017r2abstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://kavrakilab.org/publications/baker2017robonaut-2-and-you.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/ARSO.2017.8025204"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="#baker2017r2video" data-toggle="collapse"><i class="fa fa-video"></i> Video</a></p><div id="baker2017r2abstract" class="collapse"><div class="well"><p>Crew time is a precious resource due to the expense of trained human operators in space. Efficient caretaker robots could lessen the manual labor load required by frequent vehicular and life support maintenance tasks, freeing astronaut time for scientific mission objectives. Humanoid robots can fluidly exist alongside human counterparts due to their form, but they are complex and high-dimensional platforms. This paper describes a system that human operators can use to maneuver Robonaut 2 (R2), a dexterous humanoid robot developed by NASA to research co-robotic applications. The system includes a specification of constraints used to describe operations, and the supporting planning framework that solves constrained problems on R2 at interactive speeds. The paper is developed in reference to an illustrative, typical example of an operation R2 performs to highlight the challenges inherent to the problems R2 must face. Finally, the interface and planner is validated through a case-study using the guiding example on the physical robot in a simulated microgravity environment. This work reveals the complexity of employing humanoid caretaker robots and suggest solutions that are broadly applicable.</p></div><br><p><a href="#baker2017r2abstract" data-toggle="collapse">Close</a></p></br></div><div id="baker2017r2bibtex" class="collapse"><div class="well"><pre>@inproceedings{baker2017r2,
 address = {Austin, TX},
 author = {William Baker and Zachary Kingston and Mark Moll and Julia Badger and Lydia E. Kavraki},
 booktitle = {IEEE Workshop on Advanced Robotics and its Social Impacts},
 doi = {10.1109/ARSO.2017.8025204},
 month = {March},
 pages = {1--8},
 title = {Robonaut 2 and You: Specifying and Executing Complex Operations},
 year = {2017}
}</pre><br><p><a href="#baker2017r2bibtex" data-toggle="collapse">Close</a></p></br></div></div><div id="baker2017r2video" class="collapse"><div class="well"><div class="embed-responsive embed-responsive-16by9"><iframe src="https://player.vimeo.com/video/287250170?loop=1&color=ffffff&byline=0&portrait=0" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><br><p><a href="#baker2017r2video" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="dantam2016tmp"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2016</h3><dt class="col-1" style="text-align:right;">C4.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Incremental Task and Motion Planning: A Constraint-Based Approach</font><br><nobr>Neil T. Dantam</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>Swarat Chaudhuri</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#dantam2016tmpbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#dantam2016tmpabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://kavrakilab.org/publications/dantam2016tmp.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.15607/RSS.2016.XII.002"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="#dantam2016tmpvideo" data-toggle="collapse"><i class="fa fa-video"></i> Video</a></p><div id="dantam2016tmpabstract" class="collapse"><div class="well"><p>We present a new algorithm for task and motion planning (TMP) and discuss the requirements and abstractions necessary to obtain robust solutions for TMP in general. Our Iteratively Deepened Task and Motion Planning (IDTMP) method is probabilistically-complete and offers improved performance and generality compared to a similar, state-of-the-art, probabilistically-complete planner. The key idea of IDTMP is to leverage incremental constraint solving to efficiently add and remove constraints on motion feasibility at the task level. We validate IDTMP on a physical manipulator and evaluate scalability on scenarios with many objects and long plans, showing order-of-magnitude gains compared to the benchmark planner and a four-times self-comparison speedup from our extensions. Finally, in addition to describing a new method for TMP and its implementation on a physical robot, we also put forward requirements and abstractions for the development of similar planners in the future.</p></div><br><p><a href="#dantam2016tmpabstract" data-toggle="collapse">Close</a></p></br></div><div id="dantam2016tmpbibtex" class="collapse"><div class="well"><pre>@inproceedings{dantam2016tmp,
 address = {Ann Arbor, MI},
 author = {Neil T. Dantam and Zachary Kingston and Swarat Chaudhuri and Lydia E. Kavraki},
 booktitle = {Robotics: Science and Systems},
 doi = {10.15607/RSS.2016.XII.002},
 month = {June},
 title = {Incremental Task and Motion Planning: A Constraint-Based Approach},
 year = {2016}
}</pre><br><p><a href="#dantam2016tmpbibtex" data-toggle="collapse">Close</a></p></br></div></div><div id="dantam2016tmpvideo" class="collapse"><div class="well"><div class="embed-responsive embed-responsive-16by9"><iframe src="https://player.vimeo.com/video/287250215?loop=1&color=ffffff&byline=0&portrait=0" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><br><p><a href="#dantam2016tmpvideo" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="kingston2015lc3"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2015</h3><dt class="col-1" style="text-align:right;">C3.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Kinematically Constrained Workspace Control via Linear Optimization</font><br><nobr><b>Zachary Kingston</b></nobr>, <nobr>Neil T. Dantam</nobr>, and <nobr>Lydia E. Kavraki</nobr></br></p><p><a href="#kingston2015lc3bibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#kingston2015lc3abstract" data-toggle="collapse">Abstract</a><b> / </b><a href="http://kavrakilab.org/publications/kingston2015lc3.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/HUMANOIDS.2015.7363455"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="#kingston2015lc3video" data-toggle="collapse"><i class="fa fa-video"></i> Video</a></p><div id="kingston2015lc3abstract" class="collapse"><div class="well"><p>We present a method for Cartesian workspace control of a robot manipulator that enforces joint-level acceleration, velocity, and position constraints using linear optimization. This method is robust to kinematic singularities. On redundant manipulators, we avoid poor configurations near joint limits by including a maximum permissible velocity term to center each joint within its limits. Compared to the baseline Jacobian damped least-squares method of workspace control, this new approach honors kinematic limits, ensuring physically realizable control inputs and providing smoother motion of the robot. We demonstrate our method on simulated redundant and non-redundant manipulators and implement it on the physical 7-degree-of-freedom Baxter manipulator. We provide our control software under a permissive license.</p></div><br><p><a href="#kingston2015lc3abstract" data-toggle="collapse">Close</a></p></br></div><div id="kingston2015lc3bibtex" class="collapse"><div class="well"><pre>@inproceedings{kingston2015lc3,
 author = {Zachary Kingston and Neil T. Dantam and Lydia E. Kavraki},
 booktitle = {IEEE-RAS International Conference on Humanoid Robots},
 doi = {10.1109/HUMANOIDS.2015.7363455},
 month = {Nov},
 pages = {758--764},
 title = {Kinematically Constrained Workspace Control via Linear Optimization},
 year = {2015}
}</pre><br><p><a href="#kingston2015lc3bibtex" data-toggle="collapse">Close</a></p></br></div></div><div id="kingston2015lc3video" class="collapse"><div class="well"><div class="embed-responsive embed-responsive-16by9"><iframe src="https://player.vimeo.com/video/287250192?loop=1&color=ffffff&byline=0&portrait=0" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><br><p><a href="#kingston2015lc3video" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="habibi2015aamas"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">C2.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Pipelined Consensus for Global State Estimation in Multi-Agent Systems</font><br><nobr>Golnaz Habibi</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>Zijian Wang</nobr>, <nobr>Mac Schwager</nobr>, and <nobr>James McLurkin</nobr></br></p><p><a href="#habibi2015aamasbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#habibi2015aamasabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="https://zkingston.com/papers/habibi2015aamas.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.5555/2772879.2773320"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="http://dl.acm.org/citation.cfm?id=2772879.2773320"><i class="fa fa-link"></i> Publisher</a></p><div id="habibi2015aamasabstract" class="collapse"><div class="well"><p>This paper presents pipelined consensus, an extension of pair-wise gossip-based consensus, for multi-agent systems using mesh networks. Each agent starts a new consensus in each round of gossiping, and stores the intermediate results for the previous k consensus in a pipeline message. After k rounds of gossiping, the results of the first consensus are ready. The pipeline keeps each consensus independent, so any errors only persist for k rounds. This makes pipelined consensus robust to many real-world problems that other algorithms cannot handle, including message loss, changes in network topology, sensor variance, and changes in agent population. The algorithm is fully distributed and self-stabilizing, and uses a communication message of fixed size. We demonstrate the efficiency of pipelined consensus in two scenarios: computing mean sensor values in a distributed sensor network, and computing a centroid estimate in a multi-robot system. We provide extensive simulation results, and real-world experiments with up to 24 agents. The algorithm produces accurate results, and handles all of the disturbances mentioned above.</p></div><br><p><a href="#habibi2015aamasabstract" data-toggle="collapse">Close</a></p></br></div><div id="habibi2015aamasbibtex" class="collapse"><div class="well"><pre>@incollection{habibi2015aamas,
 author = {Golnaz Habibi and Zachary Kingston and Zijian Wang and Mac Schwager and James McLurkin},
 booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
 doi = {10.5555/2772879.2773320},
 isbn = {9781450334136},
 pages = {1315--1323},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 title = {Pipelined Consensus for Global State Estimation in Multi-Agent Systems},
 url = {http://dl.acm.org/citation.cfm?id=2772879.2773320},
 year = {2015}
}</pre><br><p><a href="#habibi2015aamasbibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl><a class="anchor" id="habibi2015icra"></a><dl class="row"><div class="col-lg-12 col-xl-1"></div><dt class="col-1" style="text-align:right;">C1.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Distributed Centroid Estimation and Motion Controllers for Collective Transport by Multi-Robot Systems</font><br><nobr>Golnaz Habibi</nobr>, <nobr><b>Zachary Kingston</b></nobr>, <nobr>William Xie</nobr>, <nobr>Mathew Jellins</nobr>, and <nobr>James McLurkin</nobr></br></p><p><a href="#habibi2015icrabibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="#habibi2015icraabstract" data-toggle="collapse">Abstract</a><b> / </b><a href="https://zkingston.com/papers/habibi2015icra.pdf"><i class="fa fa-file-pdf"></i> PDF</a><b> / </b><a href="https://doi.org/10.1109/ICRA.2015.7139356"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="#habibi2015icravideo" data-toggle="collapse"><i class="fa fa-video"></i> Video</a></p><div id="habibi2015icraabstract" class="collapse"><div class="well"><p>This paper presents four distributed motion controllers to enable a group of robots to collectively transport an object towards a guide robot. These controllers include: rotation around a pivot robot, rotation in-place around an estimated centroid of the object, translation, and a combined motion of rotation and translation in which each manipulating robot follows a trochoid path. Three of these controllers require an estimate of the centroid of the object, to use as the axis of rotation. Assuming the object is surrounded by manipulator robots, we approximate the centroid of the object by measuring the centroid of the manipulating robots. Our algorithms and controllers are fully distributed and robust to changes in network topology, robot population, and sensor error. We tested all of the algorithms in real-world environments with 9 robots, and show that the error of the centroid estimation is low, and that all four controllers produce reliable motion of the object.</p></div><br><p><a href="#habibi2015icraabstract" data-toggle="collapse">Close</a></p></br></div><div id="habibi2015icrabibtex" class="collapse"><div class="well"><pre>@inproceedings{habibi2015icra,
 author = {Golnaz Habibi and Zachary Kingston and William Xie and Mathew Jellins and James McLurkin},
 booktitle = {IEEE International Conference on Robotics and Automation},
 doi = {10.1109/ICRA.2015.7139356},
 pages = {1282--1288},
 title = {Distributed Centroid Estimation and Motion Controllers for Collective Transport by Multi-Robot Systems},
 year = {2015}
}</pre><br><p><a href="#habibi2015icrabibtex" data-toggle="collapse">Close</a></p></br></div></div><div id="habibi2015icravideo" class="collapse"><div class="well"><div class="embed-responsive embed-responsive-16by9"><iframe src="https://player.vimeo.com/video/287250199?loop=1&color=ffffff&byline=0&portrait=0" width="640" height="360" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe></div><br><p><a href="#habibi2015icravideo" data-toggle="collapse">Close</a></p></br></div></div></dd></dl></div><a id="bookchapters" class="anchor"></a><h2>Book Chapters</h2><div><a class="anchor" id="kingston2020book"></a><dl class="row"><h3 class="col-lg-12 col-xl-1">2020</h3><dt class="col-1" style="text-align:right;">B1.</dt><dd class="col-md-11 col-xl-10"><p><font class="title">Planning Under Manifold Constraints, </font><font class="book">Encyclopedia of Robotics</font><br><b>Zachary Kingston</b></br></p><p><a href="#kingston2020bookbibtex" data-toggle="collapse">Bibtex</a><b> / </b><a href="https://doi.org/10.1007/978-3-642-41610-1_174-1"><i class="ai ai-doi"></i> Publisher</a><b> / </b><a href="https://doi.org/10.1007/978-3-642-41610-1_174-1"><i class="fa fa-link"></i> Publisher</a></p><div id="kingston2020bookbibtex" class="collapse"><div class="well"><pre>@inbook{kingston2020book,
 address = {Berlin, Heidelberg},
 author = {Zachary Kingston},
 chapter = {Planning Under Manifold Constraints},
 doi = {10.1007/978-3-642-41610-1_174-1},
 editor = {Marcelo H. Ang and Oussama Khatib and Bruno Siciliano},
 isbn = {978-3-642-41610-1},
 pages = {1--9},
 publisher = {Springer Berlin Heidelberg},
 title = {Encyclopedia of Robotics},
 url = {https://doi.org/10.1007/978-3-642-41610-1_174-1},
 year = {2020}
}</pre><br><p><a href="#kingston2020bookbibtex" data-toggle="collapse">Close</a></p></br></div></div></dd></dl></div></div>

                </div>
                <div class="col-12 col-xs-4 col-md-3 col-lg-3 col-xl-2">
                    <div class="row">
                        <div class="col-3 col-xs-0 col-md-0 col-lg-0 col-xl-0">
                        </div>
                        <div class="col-6 col-xs-12 col-md-12 col-lg-12 col-xl-12">
                            <p>
                                <img src   = "resources/me.jpg"
                                     class = "img-fluid full-width"
                                     style = "box-shadow: 2px 4px 4px 0px;"
                                     alt   = "A picture of me from February 2020.">
                            </p>
                        </div>
                        <div class="col-3 col-xs-0 col-md-0 col-lg-0 col-xl-0">
                        </div>
                        <div class="col-3 col-xs-0 col-md-0 col-lg-0 col-xl-0">
                        </div>
                        <div class="col-6 col-xs-12 col-md-12 col-lg-12 col-xl-12">
                            <dl class="row">
                                <dt class="col-3"><a href="https://scholar.google.com/citations?user=55jqt2MAAAAJ"><i class="ai ai-google-scholar ai-2x"></i></a></dt>
                                <dt class="col-3"><a href="https://www.linkedin.com/in/zachary-kingston-79421b294/"><i class="fab fa-linkedin fa-2x"></i></a></dt>
                                <dt class="col-3"><a href="https://twitter.com/kingstonzak"><i class="fab fa-twitter fa-2x"></i></a></dt>
                                <dt class="col-3"><a href="https://github.com/zkingston"><i class="fab fa-github fa-2x"></i></a></dt>
                            </dl>
                            <dl class="row">
                                <dt class="col-3"><a href="https://orcid.org/0000-0002-3896-5110"><i class="ai ai-orcid ai-2x"></i></a></dt>
                                <dt class="col-3"><a href="https://dblp.org/pid/173/7760.html"><i class="ai ai-dblp ai-2x"></i></a></dt>
                                <dt class="col-3"><a href="https://www.researchgate.net/profile/Zachary_Kingston"><i class="ai ai-researchgate ai-2x"></i></a></dt>
                                <dt class="col-3"><a href="resources/cv_kingston.pdf"><i class="ai ai-cv ai-2x"></i></a></dt>
                            </dl>
                            <dl class="row">
                                <dt class="col-2"><i class="fa fa-envelope fa-fw blue"></i></dt>
                                <dd class="col-10">zak<i class="fa fa-at fa-xs"></i>rice.edu<br></dd>
                            </dl>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <div class="container">
            <footer>
                <p></p>
                <p style="line-height:normal;"><a href="LICENSE.txt">&copy; 2017 - 2024 Zak Kingston</a></p>
            </footer>
        </div>
        <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
    </body>
</html>
